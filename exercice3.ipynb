{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5208500e",
   "metadata": {},
   "source": [
    "## Expected Value of Empirical Risk for OLS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f115dd41",
   "metadata": {},
   "source": [
    "### OLS Setting Summary:\n",
    "- **Input space**: $X = \\mathbb{R}^d$ \n",
    "- **Output space**: $Y = \\mathbb{R}$\n",
    "- **Loss function**: Squared loss $l(y, y') = (y - y')^2$\n",
    "- **Hypothesis space**: $F = \\{x \\mapsto x^T\\theta, \\theta \\in \\mathbb{R}^d\\}$\n",
    "\n",
    "### Linear Model:\n",
    "$$y = X\\theta^* + \\epsilon$$\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ (Gaussian noise).\n",
    "\n",
    "### OLS Estimator:\n",
    "$$\\hat{\\theta} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "**Goal**: Prove that $E[R_X(\\hat{\\theta})] = \\frac{n-d}{n}\\sigma^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784c1e0",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 1 (M): Comparison with Bayes Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e493a2c2",
   "metadata": {},
   "source": [
    "The Bayes risk for the squared loss in the linear model is $\\sigma^2$ (the irreducible error due to noise).\n",
    "\n",
    "From Proposition 1: $E[R_X(\\hat{\\theta})] = \\frac{n-d}{n}\\sigma^2$\n",
    "\n",
    "**Comparison:**\n",
    "- Bayes risk: $\\sigma^2$\n",
    "- Expected empirical risk of OLS: $\\frac{n-d}{n}\\sigma^2$\n",
    "\n",
    "Since $\\frac{n-d}{n} < 1$ (assuming $d > 0$), we have:\n",
    "$$E[R_X(\\hat{\\theta})] < \\text{Bayes risk}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d4399",
   "metadata": {},
   "source": [
    "This result shows that the **expected empirical risk underestimates the true risk**.\n",
    "\n",
    "**Analysis as function of $n$ and $d$:**\n",
    "- As $n \\to \\infty$: $\\frac{n-d}{n} \\to 1$, so the bias disappears\n",
    "- As $d$ increases: The bias becomes more pronounced\n",
    "- When $d$ is close to $n$: The empirical risk severely underestimates the true risk\n",
    "\n",
    "This explains why we need separate validation/test sets to get unbiased estimates of generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc09e052",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 2 (M):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865731e5",
   "metadata": {},
   "source": [
    "We need to show:\n",
    "$$E[R_n(\\hat{\\theta})] = E_\\epsilon\\left[\\frac{1}{n}||(I_n - X(X^TX)^{-1}X^T)\\epsilon||^2\\right]$$\n",
    "\n",
    "#### Proof:\n",
    "\n",
    "Starting from the empirical risk:\n",
    "$$R_n(\\hat{\\theta}) = \\frac{1}{n}||y - X\\hat{\\theta}||^2$$\n",
    "\n",
    "Substituting $y = X\\theta^* + \\epsilon$ and $\\hat{\\theta} = (X^TX)^{-1}X^Ty$:\n",
    "\n",
    "$$y - X\\hat{\\theta} = X\\theta^* + \\epsilon - X(X^TX)^{-1}X^T(X\\theta^* + \\epsilon)$$\n",
    "\n",
    "$$= X\\theta^* + \\epsilon - X(X^TX)^{-1}X^TX\\theta^* - X(X^TX)^{-1}X^T\\epsilon$$\n",
    "\n",
    "Since $X(X^TX)^{-1}X^TX = X$ we have:\n",
    "\n",
    "$$y - X\\hat{\\theta} = X\\theta^* + \\epsilon - X\\theta^* - X(X^TX)^{-1}X^T\\epsilon$$\n",
    "\n",
    "$$= \\epsilon - X(X^TX)^{-1}X^T\\epsilon$$\n",
    "$$= (I_n - X(X^TX)^{-1}X^T)\\epsilon$$\n",
    "\n",
    "Therefore:\n",
    "$$R_n(\\hat{\\theta}) = \\frac{1}{n}||(I_n - X(X^TX)^{-1}X^T)\\epsilon||^2$$\n",
    "\n",
    "Taking expectation:\n",
    "$$E[R_n(\\hat{\\theta})] = E_\\epsilon\\left[\\frac{1}{n}||(I_n - X(X^TX)^{-1}X^T)\\epsilon||^2\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fcec0d",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 3 (M):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d508a5ca",
   "metadata": {},
   "source": [
    "$\\text{tr}$ is the trace operator, defined as $\\text{tr}(A) = \\sum_{i=1}^n A_{ii}$ for a square matrix $A$. It is a linear operator that sums the diagonal elements of a matrix.\n",
    "\n",
    "We need to show: $\\sum_{(i,j) \\in [1,n]^2} A_{ij}^2 = \\text{tr}(A^TA)$\n",
    "\n",
    "#### Proof:\n",
    "\n",
    "$$\\text{tr}(A^TA) = \\sum_{i=1}^n (A^TA)_{ii} = \\sum_{i=1}^n \\sum_{k=1}^n A_{ki}A_{ki} = \\sum_{i=1}^n \\sum_{k=1}^n A_{ki}^2$$\n",
    "\n",
    "Reindexing with $k \\to i$ and $i \\to j$:\n",
    "$$\\text{tr}(A^TA) = \\sum_{i=1}^n \\sum_{j=1}^n A_{ij}^2 = \\sum_{(i,j) \\in [1,n]^2} A_{ij}^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc51c94",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 4 (M):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be56cdb1",
   "metadata": {},
   "source": [
    "We need to show: $E_\\epsilon[\\frac{1}{n}||A\\epsilon||^2] = \\frac{\\sigma^2}{n}\\text{tr}(A^TA)$\n",
    "\n",
    "#### Proof:\n",
    "\n",
    "$\\epsilon \\in \\mathbb{R}^n$ is a vector of centered Gaussian noise with variance matrix $\\sigma^2 I_n$:\n",
    "$$\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$$\n",
    "\n",
    "We have:\n",
    "- $E[\\epsilon] = 0$\n",
    "- $\\text{Cov}(\\epsilon) = \\sigma^2 I_n$\n",
    "\n",
    "We want to compute:\n",
    "$$E_\\epsilon\\left[\\frac{1}{n}||A\\epsilon||^2\\right] = \\frac{1}{n}E_\\epsilon[\\epsilon^T A^T A \\epsilon]$$\n",
    "\n",
    "For a Gaussian centered vector $\\epsilon$, the expectation of a quadratic form is:\n",
    "$$E[\\epsilon^T B \\epsilon] = \\text{tr}(B \\cdot \\text{Cov}(\\epsilon))$$\n",
    "\n",
    "In our case, $\\text{Cov}(\\epsilon) = \\sigma^2 I_n$, so:\n",
    "$$E[\\epsilon^T A^T A \\epsilon] = \\text{tr}(A^T A \\cdot \\sigma^2 I_n)$$\n",
    "\n",
    "We can factor out $\\sigma^2$:\n",
    "$$\\text{tr}(A^T A \\cdot \\sigma^2 I_n) = \\sigma^2 \\text{tr}(A^T A)$$\n",
    "\n",
    "Then we have:\n",
    "$$E_\\epsilon\\left[\\frac{1}{n}||A\\epsilon||^2\\right] = \\frac{1}{n}\\sigma^2 \\text{tr}(A^T A) = \\frac{\\sigma^2}{n}\\text{tr}(A^T A)$$\n",
    "\n",
    "### Scientific Interpretation:\n",
    "\n",
    "This result is fundamental in linear statistics: the average variance of the projection of noise through a linear matrix $A$ depends on the trace of $A^T A$. The trace of $A^T A$ measures the \"size\" or \"complexity\" of the transformation $A$ applied to the noise, while $\\sigma^2$ reflects the intrinsic variance of the noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e02023",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 5 (M):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca872c5",
   "metadata": {},
   "source": [
    "Let $A = I_n - X(X^TX)^{-1}X^T$. We need to show $A^TA = A$.\n",
    "\n",
    "#### Proof:\n",
    "\n",
    "First, let's show that $A$ is symmetric: $A^T = A$\n",
    "\n",
    "$$A^T = (I_n - X(X^TX)^{-1}X^T)^T$$\n",
    "\n",
    "$$= I_n^T - (X(X^TX)^{-1}X^T)^T$$\n",
    "\n",
    "$$= I_n - X((X^TX)^{-1})^TX^T$$\n",
    "\n",
    "$$= I_n - X(X^TX)^{-1}X^T$$\n",
    "\n",
    "$$= A$$\n",
    "\n",
    "We have $A^T = A$, so $A$ is symmetric.\n",
    "\n",
    "Next, let's show: $A^2 = A$\n",
    "\n",
    "$$A^2 = (I_n - X(X^TX)^{-1}X^T)(I_n - X(X^TX)^{-1}X^T)$$\n",
    "\n",
    "$$= I_n - 2X(X^TX)^{-1}X^T + X(X^TX)^{-1}X^TX(X^TX)^{-1}X^T$$\n",
    "\n",
    "Since $X^TX(X^TX)^{-1} = I_d$:\n",
    "\n",
    "$$A^2 = I_n - 2X(X^TX)^{-1}X^T + X(X^TX)^{-1}X^T = I_n - X(X^TX)^{-1}X^T = A$$\n",
    "\n",
    "Since $A$ is symmetric and idempotent:\n",
    "$$A^TA = A \\cdot A = A^2 = A$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a3e75",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 6 (M):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca713c",
   "metadata": {},
   "source": [
    "Combining all previous results:\n",
    "\n",
    "From Question 2: $E[R_n(\\hat{\\theta})] = E_\\epsilon[\\frac{1}{n}||A\\epsilon||^2]$\n",
    "\n",
    "From Question 4: $E_\\epsilon[\\frac{1}{n}||A\\epsilon||^2] = \\frac{\\sigma^2}{n}\\text{tr}(A^TA)$\n",
    "\n",
    "From Question 5: $A^TA = A$\n",
    "\n",
    "Therefore: $E[R_n(\\hat{\\theta})] = \\frac{\\sigma^2}{n}\\text{tr}(A)$\n",
    "\n",
    "Now we compute $\\text{tr}(A)$:\n",
    "$$\\text{tr}(A) = \\text{tr}(I_n - X(X^TX)^{-1}X^T) = \\text{tr}(I_n) - \\text{tr}(X(X^TX)^{-1}X^T)$$\n",
    "\n",
    "$$= n - \\text{tr}((X^TX)^{-1}X^TX) = n - \\text{tr}(I_d) = n - d$$\n",
    "\n",
    "**Final result:**\n",
    "$$E[R_n(\\hat{\\theta})] = \\frac{\\sigma^2}{n}(n-d) = \\frac{n-d}{n}\\sigma^2$$\n",
    "\n",
    "This completes the proof of Proposition 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4219154",
   "metadata": {},
   "source": [
    "---\n",
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e330c",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 7 (M):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7ea3c4",
   "metadata": {},
   "source": [
    "We want to find $E\\left[\\frac{||y - X\\hat{\\theta}||^2}{n-d}\\right]$.\n",
    "\n",
    "From our previous work:\n",
    "$$||y - X\\hat{\\theta}||^2 = ||A\\epsilon||^2$$\n",
    "\n",
    "And:\n",
    "$$E[||A\\epsilon||^2] = \\sigma^2\\text{tr}(A) = \\sigma^2(n-d)$$\n",
    "\n",
    "Therefore:\n",
    "$$E\\left[\\frac{||y - X\\hat{\\theta}||^2}{n-d}\\right] = \\frac{\\sigma^2(n-d)}{n-d} = \\sigma^2$$\n",
    "\n",
    "This shows that $\\frac{||y - X\\hat{\\theta}||^2}{n-d}$ is an **unbiased estimator** of $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1254a92e",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 8 (C):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc31e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61d64129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation parameters:\n",
      "n = 100, d = 10, σ_true = 2.0\n",
      "Theoretical E[R_n(θ̂)] = 3.6000\n",
      "True σ² = 4.0000\n",
      "\n",
      "Simulation results (based on 1000 runs):\n",
      "Empirical mean of R_n(θ̂) = 3.6035\n",
      "Empirical mean of σ² estimates = 4.0039\n",
      "\n",
      "Verification:\n",
      "Theoretical vs Empirical R_n(θ̂): 3.6000 vs 3.6035\n",
      "True σ² vs Estimated σ²: 4.0000 vs 4.0039\n",
      "\n",
      "==================================================\n",
      "Testing different values of n and d:\n",
      "==================================================\n",
      "n=200, d=20:\n",
      "  Theoretical E[R_n]: 3.6000\n",
      "  Empirical E[R_n]: 3.6147\n",
      "  σ² estimate: 4.0163 (true: 4.0000)\n",
      "\n",
      "n=100, d=30:\n",
      "  Theoretical E[R_n]: 2.8000\n",
      "  Empirical E[R_n]: 2.7979\n",
      "  σ² estimate: 3.9970 (true: 4.0000)\n",
      "\n",
      "n=200, d=50:\n",
      "  Theoretical E[R_n]: 3.0000\n",
      "  Empirical E[R_n]: 2.9903\n",
      "  σ² estimate: 3.9870 (true: 4.0000)\n",
      "\n",
      "n=500, d=100:\n",
      "  Theoretical E[R_n]: 3.2000\n",
      "  Empirical E[R_n]: 3.1971\n",
      "  σ² estimate: 3.9964 (true: 4.0000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def simulate_ols_variance_estimation(n, d, sigma_true, n_simulations=1000):\n",
    "    \"\"\"\n",
    "    Simulate OLS to verify theoretical results about variance estimation.\n",
    "    \n",
    "    Parameters:\n",
    "    - n: number of samples\n",
    "    - d: number of features\n",
    "    - sigma_true: true noise standard deviation\n",
    "    - n_simulations: number of Monte Carlo simulations\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(n, d)\n",
    "    \n",
    "    # True parameter\n",
    "    theta_true = np.random.randn(d)\n",
    "    \n",
    "    empirical_risks = []\n",
    "    sigma_estimates = []\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # Generate noise\n",
    "        epsilon = np.random.normal(0, sigma_true, n)\n",
    "        \n",
    "        # Generate observations\n",
    "        y = X @ theta_true + epsilon\n",
    "    \n",
    "        # OLS estimation\n",
    "        theta_hat = np.linalg.solve(X.T @ X, X.T @ y)\n",
    "        \n",
    "        # Compute empirical risk\n",
    "        residuals = y - X @ theta_hat # Residuals: y - Xθ̂\n",
    "        empirical_risk = np.mean(residuals**2)  # R_n(θ̂) = 1/n * ||y - Xθ̂||^2\n",
    "        empirical_risks.append(empirical_risk)\n",
    "        \n",
    "        # Estimate sigma^2\n",
    "        sigma2_estimate = np.sum(residuals**2) / (n - d) # Unbiased estimate of variance: σ^2 = 1/(n-d) * ||y - Xθ̂||^2\n",
    "        sigma_estimates.append(sigma2_estimate)\n",
    "    \n",
    "    return np.array(empirical_risks), np.array(sigma_estimates)\n",
    "\n",
    "# Simulation parameters\n",
    "n = 100 # Number of samples\n",
    "d = 10 # Number of features\n",
    "sigma_true = 2.0 \n",
    "theoretical_empirical_risk = (n-d)/n * sigma_true**2\n",
    "\n",
    "print(f\"Simulation parameters:\")\n",
    "print(f\"n = {n}, d = {d}, σ_true = {sigma_true}\")\n",
    "print(f\"Theoretical E[R_n(θ̂)] = {theoretical_empirical_risk:.4f}\")\n",
    "print(f\"True σ² = {sigma_true**2:.4f}\")\n",
    "\n",
    "empirical_risks, sigma_estimates = simulate_ols_variance_estimation(n, d, sigma_true)\n",
    "print(f\"\\nSimulation results (based on {len(empirical_risks)} runs):\")\n",
    "print(f\"Empirical mean of R_n(θ̂) = {np.mean(empirical_risks):.4f}\")\n",
    "print(f\"Empirical mean of σ² estimates = {np.mean(sigma_estimates):.4f}\")\n",
    "\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Theoretical vs Empirical R_n(θ̂): {theoretical_empirical_risk:.4f} vs {np.mean(empirical_risks):.4f}\")\n",
    "print(f\"True σ² vs Estimated σ²: {sigma_true**2:.4f} vs {np.mean(sigma_estimates):.4f}\")\n",
    "\n",
    "# Test different values of n and d\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing different values of n and d:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_params = [\n",
    "    (200, 20),\n",
    "    (100, 30),\n",
    "    (200, 50),\n",
    "    (500, 100),\n",
    "]\n",
    "\n",
    "for n_test, d_test in test_params:\n",
    "    emp_risks, sigma_ests = simulate_ols_variance_estimation(n_test, d_test, sigma_true, 500)\n",
    "    theoretical = (n_test - d_test) / n_test * sigma_true**2\n",
    "    \n",
    "    print(f\"n={n_test}, d={d_test}:\")\n",
    "    print(f\"  Theoretical E[R_n]: {theoretical:.4f}\")\n",
    "    print(f\"  Empirical E[R_n]: {np.mean(emp_risks):.4f}\")\n",
    "    print(f\"  σ² estimate: {np.mean(sigma_ests):.4f} (true: {sigma_true**2:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e355e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
