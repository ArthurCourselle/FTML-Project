{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5208500e",
   "metadata": {},
   "source": [
    "## Expected Value of Empirical Risk for OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f115dd41",
   "metadata": {},
   "source": [
    "### OLS Setting Summary:\n",
    "- **Input space**: $X = \\mathbb{R}^d$ \n",
    "- **Output space**: $Y = \\mathbb{R}$\n",
    "- **Loss function**: Squared loss $l(y, y') = (y - y')^2$\n",
    "- **Hypothesis space**: $F = \\{x \\mapsto x^T\\theta, \\theta \\in \\mathbb{R}^d\\}$\n",
    "\n",
    "### Linear Model:\n",
    "$$y = X\\theta^* + \\epsilon$$\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ (Gaussian noise).\n",
    "\n",
    "### OLS Estimator:\n",
    "$$\\hat{\\theta} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "**Goal**: Prove that $E[R_X(\\hat{\\theta})] = \\frac{n-d}{n}\\sigma^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784c1e0",
   "metadata": {},
   "source": [
    "### Question 1 (M): Comparison with Bayes Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e493a2c2",
   "metadata": {},
   "source": [
    "The Bayes risk for the squared loss in the linear model is $\\sigma^2$ (the irreducible error due to noise).\n",
    "\n",
    "From Proposition 1: $E[R_X(\\hat{\\theta})] = \\frac{n-d}{n}\\sigma^2$\n",
    "\n",
    "**Comparison:**\n",
    "- Bayes risk: $\\sigma^2$\n",
    "- Expected empirical risk of OLS: $\\frac{n-d}{n}\\sigma^2$\n",
    "\n",
    "Since $\\frac{n-d}{n} < 1$ (assuming $d > 0$), we have:\n",
    "$$E[R_X(\\hat{\\theta})] < \\text{Bayes risk}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d4399",
   "metadata": {},
   "source": [
    "This result shows that the **expected empirical risk underestimates the true risk**. This phenomenon is known as **optimistic bias** of the training error.\n",
    "\n",
    "**Analysis as function of $n$ and $d$:**\n",
    "- As $n \\to \\infty$: $\\frac{n-d}{n} \\to 1$, so the bias disappears\n",
    "- As $d$ increases: The bias becomes more pronounced\n",
    "- When $d$ is close to $n$: The empirical risk severely underestimates the true risk\n",
    "\n",
    "This explains why we need separate validation/test sets to get unbiased estimates of generalization performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc09e052",
   "metadata": {},
   "source": [
    "### Question 2 (M):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865731e5",
   "metadata": {},
   "source": [
    "We need to show:\n",
    "$$E[R_n(\\hat{\\theta})] = E_\\epsilon\\left[\\frac{1}{n}||(I_n - X(X^TX)^{-1}X^T)\\epsilon||^2\\right]$$\n",
    "\n",
    "#### Proof:\n",
    "\n",
    "Starting from the empirical risk:\n",
    "$$R_n(\\hat{\\theta}) = \\frac{1}{n}||y - X\\hat{\\theta}||^2$$\n",
    "\n",
    "Substituting $y = X\\theta^* + \\epsilon$ and $\\hat{\\theta} = (X^TX)^{-1}X^Ty$:\n",
    "\n",
    "$$y - X\\hat{\\theta} = X\\theta^* + \\epsilon - X(X^TX)^{-1}X^T(X\\theta^* + \\epsilon)$$\n",
    "\n",
    "$$= X\\theta^* + \\epsilon - X(X^TX)^{-1}X^TX\\theta^* - X(X^TX)^{-1}X^T\\epsilon$$\n",
    "\n",
    "Since $X(X^TX)^{-1}X^TX = X$ (assuming $X$ has full column rank):\n",
    "\n",
    "$$y - X\\hat{\\theta} = X\\theta^* + \\epsilon - X\\theta^* - X(X^TX)^{-1}X^T\\epsilon$$\n",
    "\n",
    "$$= \\epsilon - X(X^TX)^{-1}X^T\\epsilon = (I_n - X(X^TX)^{-1}X^T)\\epsilon$$\n",
    "\n",
    "Therefore:\n",
    "$$R_n(\\hat{\\theta}) = \\frac{1}{n}||(I_n - X(X^TX)^{-1}X^T)\\epsilon||^2$$\n",
    "\n",
    "Taking expectation:\n",
    "$$E[R_n(\\hat{\\theta})] = E_\\epsilon\\left[\\frac{1}{n}||(I_n - X(X^TX)^{-1}X^T)\\epsilon||^2\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fcec0d",
   "metadata": {},
   "source": [
    "### Question 3 (M):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d508a5ca",
   "metadata": {},
   "source": [
    "\n",
    "We need to show: $\\sum_{(i,j) \\in [1,n]^2} A_{ij}^2 = \\text{tr}(A^TA)$\n",
    "\n",
    "#### Proof:\n",
    "\n",
    "$$\\text{tr}(A^TA) = \\sum_{i=1}^n (A^TA)_{ii} = \\sum_{i=1}^n \\sum_{k=1}^n A_{ki}A_{ki} = \\sum_{i=1}^n \\sum_{k=1}^n A_{ki}^2$$\n",
    "\n",
    "Reindexing with $k \\to i$ and $i \\to j$:\n",
    "$$\\text{tr}(A^TA) = \\sum_{i=1}^n \\sum_{j=1}^n A_{ij}^2 = \\sum_{(i,j) \\in [1,n]^2} A_{ij}^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc51c94",
   "metadata": {},
   "source": [
    "### Question 4 (M):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02df124",
   "metadata": {},
   "source": [
    "\n",
    "We need to show: $E_\\epsilon[\\frac{1}{n}||A\\epsilon||^2] = \\frac{\\sigma^2}{n}\\text{tr}(A^TA)$\n",
    "\n",
    "#### Proof:\n",
    "\n",
    "$$E_\\epsilon[||A\\epsilon||^2] = E_\\epsilon[\\sum_{i=1}^n (A\\epsilon)_i^2] = E_\\epsilon[\\sum_{i=1}^n (\\sum_{j=1}^n A_{ij}\\epsilon_j)^2]$$\n",
    "\n",
    "$$= E_\\epsilon[\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{k=1}^n A_{ij}A_{ik}\\epsilon_j\\epsilon_k]$$\n",
    "\n",
    "Since $E[\\epsilon_j\\epsilon_k] = \\sigma^2\\delta_{jk}$ (where $\\delta_{jk}$ is the Kronecker delta):\n",
    "\n",
    "$$E_\\epsilon[||A\\epsilon||^2] = \\sum_{i=1}^n \\sum_{j=1}^n A_{ij}A_{ij}\\sigma^2 = \\sigma^2\\sum_{i=1}^n \\sum_{j=1}^n A_{ij}^2$$\n",
    "\n",
    "Using Question 3:\n",
    "$$E_\\epsilon[||A\\epsilon||^2] = \\sigma^2 \\text{tr}(A^TA)$$\n",
    "\n",
    "Therefore:\n",
    "$$E_\\epsilon[\\frac{1}{n}||A\\epsilon||^2] = \\frac{\\sigma^2}{n}\\text{tr}(A^TA)$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
